{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318c909d-987b-4872-b0a7-4b051a52325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import multiprocessing\n",
    "import random\n",
    "\n",
    "#Visualizations\n",
    "import os\n",
    "\n",
    "#Statistics packages\n",
    "from scipy.stats import pearsonr \n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_recall_curve, auc, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Others\n",
    "#import tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004d333-a309-41e9-9e92-075fd06829d0",
   "metadata": {},
   "source": [
    "## Import all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b117e77c-a387-46ea-b4d4-19c5645c798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlayerPos = pd.read_csv('PlayerPos.csv')\n",
    "Home = pd.read_csv('Home.csv')\n",
    "Away = pd.read_csv('Away.csv')\n",
    "VelHome = pd.read_csv('VelHome.csv')\n",
    "VelAway = pd.read_csv('VelAway.csv')\n",
    "DiscAccHome = pd.read_csv('DiscAccHome.csv')\n",
    "DiscAccAway = pd.read_csv('DiscAccAway.csv')\n",
    "Ball = pd.read_csv('Ball.csv')\n",
    "VelBall = pd.read_csv('VelBall.csv')\n",
    "DefRewardList = pd.read_csv('DefRewardListnd3.csv')\n",
    "AttRewardList = pd.read_csv('AttRewardListnd3.csv')\n",
    "ValDefRewardList = pd.read_csv('ValDefRewardListnd3.csv')\n",
    "ValAttRewardList = pd.read_csv('ValAttRewardListnd3.csv')\n",
    "DefEvaluationList = pd.read_csv('DefEvaluationList.csv')\n",
    "AttEvaluationList = pd.read_csv('AttEvaluationList.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8595aad-2147-42c8-896a-c465e2b16ca4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Single run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978cfe27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRewardList = pd.read_csv(\\'RewardList.csv\\')\\n\\nRewardList[\\'Reward\\'] = RewardList[\\'Reward\\'].shift(-1)\\n\\nNewRewardList = []\\nfor i in range(1,3):\\n    for j in range(1,12):\\n        k = 0\\n        while k < len(RewardList):\\n            FirstTouch = False\\n            if RewardList.iloc[k][3] != j or RewardList.iloc[k][0] == 0 or RewardList.iloc[k][2] != i:\\n                NewRewardList.append([RewardList.iloc[k][0],RewardList.iloc[k][1],RewardList.iloc[k][2],RewardList.iloc[k][3],i,j,RewardList.iloc[k][4]])\\n                k += 1\\n            else:\\n                if RewardList.iloc[k-1][0] == 0:\\n                    FirstTouch = True\\n                while RewardList.iloc[k][3] == j and RewardList.iloc[k][2] == i:\\n                    k += 1\\n                if RewardList.iloc[k][0] != 0 and not FirstTouch:\\n                    NewRewardList.append([0,\\'Ball Received\\',i,j,0,0,0])\\n\\nPlayerRewardList = pd.DataFrame(NewRewardList,columns = [\\'Match_index\\', \\'Frame\\',\\'Team\\',\\'Player\\',\\'Agent Team\\',\\'Agent Player\\',\\'Reward\\'])\\n\\nPlayerRewardList.to_csv(\\'PlayerRewardList.csv\\',header = True, index = False)\\n\\nRewardList = pd.read_csv(\\'PlayerRewardList.csv\\')\\n\\nDefRewardList = RewardList[RewardList[\\'Team\\'] != RewardList[\\'Agent Team\\']]\\n\\ni = 0\\nwhile i < len(DefRewardList):\\n    if i == 0 and DefRewardList[\\'Match_index\\'].iloc[i] == 0:\\n        DefRewardList = DefRewardList.drop(DefRewardList.index[i])\\n    else:\\n        while DefRewardList[\\'Match_index\\'].iloc[i] != 0:\\n            i += 1\\n        i += 1\\n        while i < len(DefRewardList) and DefRewardList[\\'Match_index\\'].iloc[i] == 0:\\n            DefRewardList = DefRewardList.drop(DefRewardList.index[i])\\n    \\n\\nDefRewardList[\\'Reward\\'] = DefRewardList[\\'Reward\\'].multiply(-1)\\n\\nDefRewardList.to_csv(\\'DefRewardList.csv\\', index = False)\\n\\nDefRewardList = pd.read_csv(\\'DefRewardList.csv\\')\\n\\nAttRewardList = RewardList[(RewardList[\\'Team\\'] == RewardList[\\'Agent Team\\']) | (RewardList[\\'Match_index\\']==0)]\\n\\ni = 0\\nwhile i < len(AttRewardList):\\n    if i == 0 and AttRewardList[\\'Match_index\\'].iloc[i] == 0:\\n        AttRewardList = AttRewardList.drop(AttRewardList.index[i])\\n    else:\\n        while AttRewardList[\\'Match_index\\'].iloc[i] != 0:\\n            i += 1\\n        i += 1\\n        while i < len(AttRewardList) and AttRewardList[\\'Match_index\\'].iloc[i] == 0:\\n            AttRewardList = AttRewardList.drop(AttRewardList.index[i])\\n    \\n\\nAttRewardList.to_csv(\\'AttRewardList.csv\\', index = False)\\n\\nAttRewardList = pd.read_csv(\\'AttRewardList.csv\\')\\n\\ndef get_cardinal_direction(vector):\\n    # Define cardinal directions\\n    directions = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\\n\\n    # Calculate angle and magnitude\\n    angle = math.atan2(vector[0], vector[1])\\n    angle_degrees = math.degrees(angle)\\n    magnitude = math.sqrt(vector[0] ** 2 + vector[1] ** 2)\\n\\n    # Find the closest cardinal direction\\n    index = round(angle_degrees / 45) % 8\\n    closest_direction = directions[index]\\n\\n    return closest_direction, magnitude\\nAccHome = pd.read_csv(\\'AccHome.csv\\')\\nAccAway = pd.read_csv(\\'AccAway.csv\\')\\n\\ni = 0\\nwhile i < len(AccHome):\\n    j = 1\\n    while j < 12:\\n        Direction, Magnitude = get_cardinal_direction([AccHome.iloc[i,2*j],AccHome.iloc[i,2*j + 1]])\\n        AccHome.iloc[i,2*j] = Magnitude\\n        AccHome.iloc[i,2*j + 1] = Direction\\n        j += 1\\n    i += 1\\n\\n\\nAccHome.to_csv(\\'DiscAccHome.csv\\', index = False)\\n\\ni = 0\\nwhile i < len(AccAway):\\n    j = 1\\n    while j < 12:\\n        Direction, Magnitude = get_cardinal_direction([AccAway.iloc[i,2*j],AccAway.iloc[i,2*j + 1]])\\n        AccAway.iloc[i,2*j] = Magnitude\\n        AccAway.iloc[i,2*j + 1] = Direction\\n        j += 1\\n    i += 1\\n\\n\\nAccAway.to_csv(\\'DiscAccAway.csv\\', index = False)\\n\\n#AccAway.describe()\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "RewardList = pd.read_csv('RewardList.csv')\n",
    "\n",
    "RewardList['Reward'] = RewardList['Reward'].shift(-1)\n",
    "\n",
    "NewRewardList = []\n",
    "for i in range(1,3):\n",
    "    for j in range(1,12):\n",
    "        k = 0\n",
    "        while k < len(RewardList):\n",
    "            FirstTouch = False\n",
    "            if RewardList.iloc[k][3] != j or RewardList.iloc[k][0] == 0 or RewardList.iloc[k][2] != i:\n",
    "                NewRewardList.append([RewardList.iloc[k][0],RewardList.iloc[k][1],RewardList.iloc[k][2],RewardList.iloc[k][3],i,j,RewardList.iloc[k][4]])\n",
    "                k += 1\n",
    "            else:\n",
    "                if RewardList.iloc[k-1][0] == 0:\n",
    "                    FirstTouch = True\n",
    "                while RewardList.iloc[k][3] == j and RewardList.iloc[k][2] == i:\n",
    "                    k += 1\n",
    "                if RewardList.iloc[k][0] != 0 and not FirstTouch:\n",
    "                    NewRewardList.append([0,'Ball Received',i,j,0,0,0])\n",
    "\n",
    "PlayerRewardList = pd.DataFrame(NewRewardList,columns = ['Match_index', 'Frame','Team','Player','Agent Team','Agent Player','Reward'])\n",
    "\n",
    "PlayerRewardList.to_csv('PlayerRewardList.csv',header = True, index = False)\n",
    "\n",
    "RewardList = pd.read_csv('PlayerRewardList.csv')\n",
    "\n",
    "DefRewardList = RewardList[RewardList['Team'] != RewardList['Agent Team']]\n",
    "\n",
    "i = 0\n",
    "while i < len(DefRewardList):\n",
    "    if i == 0 and DefRewardList['Match_index'].iloc[i] == 0:\n",
    "        DefRewardList = DefRewardList.drop(DefRewardList.index[i])\n",
    "    else:\n",
    "        while DefRewardList['Match_index'].iloc[i] != 0:\n",
    "            i += 1\n",
    "        i += 1\n",
    "        while i < len(DefRewardList) and DefRewardList['Match_index'].iloc[i] == 0:\n",
    "            DefRewardList = DefRewardList.drop(DefRewardList.index[i])\n",
    "    \n",
    "\n",
    "DefRewardList['Reward'] = DefRewardList['Reward'].multiply(-1)\n",
    "\n",
    "DefRewardList.to_csv('DefRewardList.csv', index = False)\n",
    "\n",
    "DefRewardList = pd.read_csv('DefRewardList.csv')\n",
    "\n",
    "AttRewardList = RewardList[(RewardList['Team'] == RewardList['Agent Team']) | (RewardList['Match_index']==0)]\n",
    "\n",
    "i = 0\n",
    "while i < len(AttRewardList):\n",
    "    if i == 0 and AttRewardList['Match_index'].iloc[i] == 0:\n",
    "        AttRewardList = AttRewardList.drop(AttRewardList.index[i])\n",
    "    else:\n",
    "        while AttRewardList['Match_index'].iloc[i] != 0:\n",
    "            i += 1\n",
    "        i += 1\n",
    "        while i < len(AttRewardList) and AttRewardList['Match_index'].iloc[i] == 0:\n",
    "            AttRewardList = AttRewardList.drop(AttRewardList.index[i])\n",
    "    \n",
    "\n",
    "AttRewardList.to_csv('AttRewardList.csv', index = False)\n",
    "\n",
    "AttRewardList = pd.read_csv('AttRewardList.csv')\n",
    "\n",
    "def get_cardinal_direction(vector):\n",
    "    # Define cardinal directions\n",
    "    directions = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n",
    "\n",
    "    # Calculate angle and magnitude\n",
    "    angle = math.atan2(vector[0], vector[1])\n",
    "    angle_degrees = math.degrees(angle)\n",
    "    magnitude = math.sqrt(vector[0] ** 2 + vector[1] ** 2)\n",
    "\n",
    "    # Find the closest cardinal direction\n",
    "    index = round(angle_degrees / 45) % 8\n",
    "    closest_direction = directions[index]\n",
    "\n",
    "    return closest_direction, magnitude\n",
    "AccHome = pd.read_csv('AccHome.csv')\n",
    "AccAway = pd.read_csv('AccAway.csv')\n",
    "\n",
    "i = 0\n",
    "while i < len(AccHome):\n",
    "    j = 1\n",
    "    while j < 12:\n",
    "        Direction, Magnitude = get_cardinal_direction([AccHome.iloc[i,2*j],AccHome.iloc[i,2*j + 1]])\n",
    "        AccHome.iloc[i,2*j] = Magnitude\n",
    "        AccHome.iloc[i,2*j + 1] = Direction\n",
    "        j += 1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "AccHome.to_csv('DiscAccHome.csv', index = False)\n",
    "\n",
    "i = 0\n",
    "while i < len(AccAway):\n",
    "    j = 1\n",
    "    while j < 12:\n",
    "        Direction, Magnitude = get_cardinal_direction([AccAway.iloc[i,2*j],AccAway.iloc[i,2*j + 1]])\n",
    "        AccAway.iloc[i,2*j] = Magnitude\n",
    "        AccAway.iloc[i,2*j + 1] = Direction\n",
    "        j += 1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "AccAway.to_csv('DiscAccAway.csv', index = False)\n",
    "\n",
    "#AccAway.describe()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17dc9322",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## Code for non discounted dataset###\\nValAttRewardList = AttRewardList.loc[(AttRewardList['Match_index']==3) | (AttRewardList['Match_index'] == 0)]\\n\\ni = 0\\nwhile i < len(ValAttRewardList):\\n    if ValAttRewardList.iloc[i][0] == 3:\\n        i += 1\\n    else:\\n        if i != 0:\\n            i += 1\\n        while i < len(ValAttRewardList) and ValAttRewardList.iloc[i][0] == 0:\\n            ValAttRewardList = ValAttRewardList.drop(ValAttRewardList.index[i])\\n        \\n\\nAttRewardList = AttRewardList.drop(ValAttRewardList.index)\\n\\nValDefRewardList = DefRewardList.loc[(DefRewardList['Match_index']==3) | (DefRewardList['Match_index'] == 0)]\\n\\ni = 0\\nwhile i < len(ValDefRewardList):\\n    if ValDefRewardList.iloc[i][0] == 3:\\n        i += 1\\n    else:\\n        if i != 0:\\n            i += 1\\n        while i < len(ValDefRewardList) and ValDefRewardList.iloc[i][0] == 0:\\n            ValDefRewardList = ValDefRewardList.drop(ValDefRewardList.index[i])\\n        \\n\\nDefRewardList = DefRewardList.drop(ValDefRewardList.index)\\n\\nAttRewardList.to_csv('AttRewardListnd3.csv', index = False)\\nDefRewardList.to_csv('DefRewardListnd3.csv', index = False)\\nValAttRewardList.to_csv('ValAttRewardListn3.csv', index = False)\\nValDefRewardList.to_csv('ValDefRewardListnd3.csv', index = False)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## Code for non discounted dataset###\n",
    "ValAttRewardList = AttRewardList.loc[(AttRewardList['Match_index']==3) | (AttRewardList['Match_index'] == 0)]\n",
    "\n",
    "i = 0\n",
    "while i < len(ValAttRewardList):\n",
    "    if ValAttRewardList.iloc[i][0] == 3:\n",
    "        i += 1\n",
    "    else:\n",
    "        if i != 0:\n",
    "            i += 1\n",
    "        while i < len(ValAttRewardList) and ValAttRewardList.iloc[i][0] == 0:\n",
    "            ValAttRewardList = ValAttRewardList.drop(ValAttRewardList.index[i])\n",
    "        \n",
    "\n",
    "AttRewardList = AttRewardList.drop(ValAttRewardList.index)\n",
    "\n",
    "ValDefRewardList = DefRewardList.loc[(DefRewardList['Match_index']==3) | (DefRewardList['Match_index'] == 0)]\n",
    "\n",
    "i = 0\n",
    "while i < len(ValDefRewardList):\n",
    "    if ValDefRewardList.iloc[i][0] == 3:\n",
    "        i += 1\n",
    "    else:\n",
    "        if i != 0:\n",
    "            i += 1\n",
    "        while i < len(ValDefRewardList) and ValDefRewardList.iloc[i][0] == 0:\n",
    "            ValDefRewardList = ValDefRewardList.drop(ValDefRewardList.index[i])\n",
    "        \n",
    "\n",
    "DefRewardList = DefRewardList.drop(ValDefRewardList.index)\n",
    "\n",
    "AttRewardList.to_csv('AttRewardListnd3.csv', index = False)\n",
    "DefRewardList.to_csv('DefRewardListnd3.csv', index = False)\n",
    "ValAttRewardList.to_csv('ValAttRewardListn3.csv', index = False)\n",
    "ValDefRewardList.to_csv('ValDefRewardListnd3.csv', index = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b75d0b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPlayerRewardList.insert(len(PlayerRewardList.columns),'First_state', np.zeros(len(PlayerRewardList)),True)\\n\\ni = 0\\nwhile i < len(PlayerRewardList):\\n    if i == 0:\\n        PlayerRewardList.iloc[i,-1] = 1\\n        i += 1\\n    else:\\n        if PlayerRewardList.iloc[i-1,0] == 0:\\n            PlayerRewardList.iloc[i,-1] = 1\\n        i += 1\\n\\nEvaluationList = PlayerRewardList.loc[PlayerRewardList['Match_index'] != 0]\\nDefEvaluationList = EvaluationList[EvaluationList['Team'] != EvaluationList['Agent Team']].reset_index(drop = True)\\nAttEvaluationList = EvaluationList[EvaluationList['Team'] == EvaluationList['Agent Team']].reset_index(drop = True)\\n\\nDefEvaluationList.to_csv('DefEvaluationList.csv',header = True, index = False)\\nAttEvaluationList.to_csv('AttEvaluationList.csv',header = True, index = False)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PlayerRewardList.insert(len(PlayerRewardList.columns),'First_state', np.zeros(len(PlayerRewardList)),True)\n",
    "\n",
    "i = 0\n",
    "while i < len(PlayerRewardList):\n",
    "    if i == 0:\n",
    "        PlayerRewardList.iloc[i,-1] = 1\n",
    "        i += 1\n",
    "    else:\n",
    "        if PlayerRewardList.iloc[i-1,0] == 0:\n",
    "            PlayerRewardList.iloc[i,-1] = 1\n",
    "        i += 1\n",
    "\n",
    "EvaluationList = PlayerRewardList.loc[PlayerRewardList['Match_index'] != 0]\n",
    "DefEvaluationList = EvaluationList[EvaluationList['Team'] != EvaluationList['Agent Team']].reset_index(drop = True)\n",
    "AttEvaluationList = EvaluationList[EvaluationList['Team'] == EvaluationList['Agent Team']].reset_index(drop = True)\n",
    "\n",
    "DefEvaluationList.to_csv('DefEvaluationList.csv',header = True, index = False)\n",
    "AttEvaluationList.to_csv('AttEvaluationList.csv',header = True, index = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e134a",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8187372-d592-4371-86d6-53d4c37cd76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrameToIndex(Match_index, Frame): #A function that receives a frame and outputs the corresponding dataframe index\n",
    "    return Ball.loc[(Ball['Frame']==int(Frame)) & (Ball['Match_index'] == Match_index)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688b3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningDataGenerator(Dataset):\n",
    "    def __init__(self, reward_list):\n",
    "        self.reward_list = reward_list\n",
    "        self.No_zeros = self.reward_list.loc[self.reward_list['Match_index'] != 0]\n",
    "        self.eval_flag = len(self.reward_list) == len(self.No_zeros)\n",
    "        # Load dataFrames outside the constructor for efficiency\n",
    "        #self.PlayerPos = pd.read_csv('PlayerPos.csv')\n",
    "        #self.Home = pd.read_csv('Home.csv')\n",
    "        #self.Away = pd.read_csv('Away.csv')\n",
    "        #self.VelHome = pd.read_csv('VelHome.csv')\n",
    "        #self.VelAway = pd.read_csv('VelAway.csv')\n",
    "        #self.AccHome = pd.read_csv('DiscAccHome.csv')\n",
    "        #self.AccAway = pd.read_csv('DiscAccAway.csv')\n",
    "        #self.Ball = pd.read_csv('Ball.csv')\n",
    "        #self.VelBall = pd.read_csv('VelBall.csv')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.No_zeros)\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.__len__() - idx - 1\n",
    "        Match_index, Frame, Team, Player, AgentTeam, AgentPlayer, reward = self.No_zeros.iloc[idx]\n",
    "        original_id = self.No_zeros.index[idx]\n",
    "        if self.eval_flag:\n",
    "            final_state = False\n",
    "        else:\n",
    "            final_state = self.reward_list.iloc[original_id + 1][0] == 0  # Check next row for episode end\n",
    "        state = self.get_state(Match_index, Frame, Team, Player, AgentTeam, AgentPlayer, final_state)\n",
    "        action = self.get_action(Match_index, Frame, Team, Player, AgentTeam, AgentPlayer)\n",
    "        return state, action, reward\n",
    "\n",
    "    def get_state(self, Match_index, Frame,Team,Player,AgentTeam,AgentPlayer, final_state):\n",
    "        AgentPlayer = int(AgentPlayer) #Fixes bug with evaluation dataframe\n",
    "        n = FrameToIndex(Match_index, Frame)\n",
    "        MatOpp = np.zeros([60,100])\n",
    "        MatTeam = np.zeros([60,100])\n",
    "        MatOppvx = np.zeros([60,100])\n",
    "        MatOppvy = np.zeros([60,100])\n",
    "        MatTeamvx = np.zeros([60,100])\n",
    "        MatTeamvy = np.zeros([60,100])\n",
    "        MatBall = np.zeros([60,100])\n",
    "        MatBallvx = np.zeros([60,100])\n",
    "        MatBallvy = np.zeros([60,100])\n",
    "        MatAge = np.zeros([60,100])\n",
    "        MatAgevx = np.zeros([60,100])\n",
    "        MatAgevy = np.zeros([60,100])\n",
    "        xball = Ball.iloc[n,2]\n",
    "        yball = Ball.iloc[n,3]\n",
    "        MatBall[yball][xball] = 1\n",
    "        MatBallvx[yball][xball] = VelBall.iloc[n,2]\n",
    "        MatBallvy[yball][xball] = VelBall.iloc[n,3]\n",
    "        if AgentTeam==1:\n",
    "            yopp = Away.iloc[n, [2*i+2 for i in range(1, 12)]].to_numpy()\n",
    "            xopp = Away.iloc[n, [2*i+1 for i in range(1, 12)]].to_numpy()\n",
    "            yteam = Home.iloc[n, [2*i+2 for i in range(1, 12) if i != AgentPlayer]].to_numpy()\n",
    "            xteam = Home.iloc[n, [2*i+1 for i in range(1, 12) if i != AgentPlayer]].to_numpy()\n",
    "            yopp = yopp.round().astype(int)  # Round and convert to integer\n",
    "            xopp = xopp.round().astype(int)\n",
    "            yteam = yteam.round().astype(int)\n",
    "            xteam = xteam.round().astype(int)\n",
    "            xvelteam = VelHome.iloc[n,[2*i for i in range(1,12) if i != AgentPlayer]].to_numpy()\n",
    "            yvelteam = VelHome.iloc[n,[2*i+1 for i in range(1,12) if i != AgentPlayer]].to_numpy()\n",
    "            xvelopp = VelAway.iloc[n,[2*i for i in range(1,12)]].to_numpy()\n",
    "            yvelopp = VelAway.iloc[n,[2*i+1 for i in range(1,12)]].to_numpy()\n",
    "            MatOpp[yopp, xopp] = 1\n",
    "            MatOppvx[yopp, xopp] = xvelopp\n",
    "            MatOppvy[yopp, xopp] = yvelopp\n",
    "            MatTeam[yteam, xteam] = 1\n",
    "            MatTeamvx[yteam, xteam] = xvelteam\n",
    "            MatTeamvy[yteam, xteam] = yvelteam\n",
    "            yage = Home.iloc[n,2*AgentPlayer + 2]\n",
    "            xage = Home.iloc[n,2*AgentPlayer + 1]\n",
    "            xvelage = VelHome.iloc[n,2*AgentPlayer]\n",
    "            yvelage = VelHome.iloc[n,2*AgentPlayer + 1]\n",
    "            MatAge[yage, xage] = 1\n",
    "            MatAgevx[yage, xage] = xvelage\n",
    "            MatAgevy[yage, xage] = yvelage\n",
    "        if AgentTeam==2:\n",
    "            yopp = Home.iloc[n, [2*i+2 for i in range(1, 12)]].to_numpy()\n",
    "            xopp = Home.iloc[n, [2*i+1 for i in range(1, 12)]].to_numpy()\n",
    "            yteam = Away.iloc[n, [2*i+2 for i in range(1, 12) if i != AgentPlayer]].to_numpy()\n",
    "            xteam = Away.iloc[n, [2*i+1 for i in range(1, 12) if i != AgentPlayer]].to_numpy()\n",
    "            yopp = yopp.round().astype(int)  # Round and convert to integer\n",
    "            xopp = xopp.round().astype(int)\n",
    "            yteam = yteam.round().astype(int)\n",
    "            xteam = xteam.round().astype(int)\n",
    "            xvelteam = VelAway.iloc[n,[2*i for i in range(1,12) if i != AgentPlayer]].to_numpy()\n",
    "            yvelteam = VelAway.iloc[n,[2*i+1 for i in range(1,12) if i != AgentPlayer]].to_numpy()\n",
    "            xvelopp = VelHome.iloc[n,[2*i for i in range(1,12)]].to_numpy()\n",
    "            yvelopp = VelHome.iloc[n,[2*i+1 for i in range(1,12)]].to_numpy()\n",
    "            MatOpp[yopp, xopp] = 1\n",
    "            MatOppvx[yopp, xopp] = xvelopp\n",
    "            MatOppvy[yopp, xopp] = yvelopp\n",
    "            MatTeam[yteam, xteam] = 1\n",
    "            MatTeamvx[yteam, xteam] = xvelteam\n",
    "            MatTeamvy[yteam, xteam] = yvelteam\n",
    "            yage = Away.iloc[n,2*AgentPlayer + 2]\n",
    "            xage = Away.iloc[n,2*AgentPlayer + 1]\n",
    "            xvelage = VelAway.iloc[n,2*AgentPlayer]\n",
    "            yvelage = VelAway.iloc[n,2*AgentPlayer + 1]\n",
    "            MatAge[yage, xage] = 1\n",
    "            MatAgevx[yage, xage] = xvelage\n",
    "            MatAgevy[yage, xage] = yvelage\n",
    "        MatOpp = torch.from_numpy(MatOpp)\n",
    "        MatTeam = torch.from_numpy(MatTeam)\n",
    "        MatOppvx = torch.from_numpy(MatOppvx)\n",
    "        MatOppvy = torch.from_numpy(MatOppvy)\n",
    "        MatTeamvx = torch.from_numpy(MatTeamvx)\n",
    "        MatTeamvy = torch.from_numpy(MatTeamvy)\n",
    "        MatBall = torch.from_numpy(MatBall)\n",
    "        MatBallvx = torch.from_numpy(MatBallvx)\n",
    "        MatBallvy = torch.from_numpy(MatBallvy)\n",
    "        MatAge = torch.from_numpy(MatAge)\n",
    "        MatAgevx = torch.from_numpy(MatAgevx)\n",
    "        MatAgevy = torch.from_numpy(MatAgevy)\n",
    "        return [torch.stack([MatOpp,MatTeam,MatOppvx,MatOppvy,MatTeamvx,MatTeamvy,MatBall,MatBallvx,MatBallvy,MatAge,MatAgevx,MatAgevy]), final_state]\n",
    "\n",
    "\n",
    "  # Add a method to get action if applicable for your Q-learning approach\n",
    "    def get_action(self, Match_index, Frame, Team,Player,AgentTeam,AgentPlayer):\n",
    "        AgentPlayer = int(AgentPlayer)\n",
    "        n = FrameToIndex(Match_index, Frame)\n",
    "        if AgentTeam==1:\n",
    "            Magnitude = DiscAccHome.iloc[n,2*AgentPlayer]\n",
    "            Direction = DiscAccHome.iloc[n,2*AgentPlayer+1]\n",
    "        if AgentTeam==2:\n",
    "            Magnitude = DiscAccAway.iloc[n,2*AgentPlayer]\n",
    "            Direction = DiscAccAway.iloc[n,2*AgentPlayer+1]\n",
    "        if Magnitude < 0.2:\n",
    "            return 'No Acceleration'\n",
    "        elif Magnitude < 0.5:\n",
    "            return 'Small, '+Direction\n",
    "        elif Magnitude < 1:\n",
    "            return 'Medium, '+Direction\n",
    "        else:\n",
    "            return 'Big, '+Direction\n",
    "        return (Magnitude,Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751c013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_generator = QLearningDataGenerator(DefRewardList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22219d73-a1f7-4738-9b1e-3816b6a3662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_generator = QLearningDataGenerator(AttRewardList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee95673",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_def_generator = QLearningDataGenerator(ValDefRewardList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59263a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_att_generator = QLearningDataGenerator(ValAttRewardList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1faccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_def_generator = QLearningDataGenerator(DefEvaluationList.drop(['First_state'],axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a014bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_att_generator = QLearningDataGenerator(AttEvaluationList.drop(['First_state'],axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2d96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_dataloader = DataLoader(def_generator, batch_size=1, shuffle=False, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ce9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_dataloader = DataLoader(att_generator, batch_size=1, shuffle=False, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d50a339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_def_dataloader = DataLoader(val_def_generator, batch_size=1, shuffle=False, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f4ee3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_att_dataloader = DataLoader(val_att_generator, batch_size=1, shuffle=False, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7864d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_def_dataloader = DataLoader(eval_def_generator, batch_size=1, shuffle=False, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36360269",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_att_dataloader = DataLoader(eval_att_generator, batch_size=1, shuffle=False, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b6cd36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(1,13):\\n    att_dataloader = DataLoader(att_generator, batch_size=1, shuffle=False, num_workers = i)\\n    j = 0\\n    start_time = time.time()\\n    for states, actions, rewards in att_dataloader:\\n        j+=1\\n        if j == 1000000:\\n            break\\n    print('For ',13-i,'workers, it takes ', time.time()-start_time, 'seconds')\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(1,13):\n",
    "    att_dataloader = DataLoader(att_generator, batch_size=1, shuffle=False, num_workers = i)\n",
    "    j = 0\n",
    "    start_time = time.time()\n",
    "    for states, actions, rewards in att_dataloader:\n",
    "        j+=1\n",
    "        if j == 1000000:\n",
    "            break\n",
    "    print('For ',13-i,'workers, it takes ', time.time()-start_time, 'seconds')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534479b",
   "metadata": {},
   "source": [
    "For  1 workers, it takes  244.71065592765808 seconds\n",
    "\n",
    "For  2 workers, it takes  139.8216598033905 seconds\n",
    "\n",
    "For  3 workers, it takes  105.91393327713013 seconds\n",
    "\n",
    "For  4 workers, it takes  89.58012199401855 seconds\n",
    "\n",
    "For  5 workers, it takes  82.76936197280884 seconds\n",
    "\n",
    "For  6 workers, it takes  79.70292806625366 seconds\n",
    "\n",
    "For  7 workers, it takes  80.15350341796875 seconds\n",
    "\n",
    "For  8 workers, it takes  81.66234469413757 seconds\n",
    "\n",
    "For  9 workers, it takes  82.12781977653503 seconds\n",
    "\n",
    "For  10 workers, it takes  83.10064005851746 seconds\n",
    "\n",
    "For  11 workers, it takes  83.56236934661865 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394cbf8a",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef7bb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers with max pooling and ReLU activation\n",
    "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3)\n",
    "        #self.norm1 = nn.BatchNorm2d(12)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=2)\n",
    "        #self.norm2 = nn.BatchNorm2d(12)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #self.conv3 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3)\n",
    "        #self.norm3 = nn.BatchNorm2d(12)\n",
    "        #self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        #self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Flatten the output of convolutional layers\n",
    "        #self.flatten = torch.flatten()\n",
    "\n",
    "        # Hidden layers with sigmoid activation\n",
    "        self.fc1 = nn.Linear(12*14*24, 256)\n",
    "        #self.fc1 = nn.Linear(12*6*11, 256)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        #self.fc2 = nn.Linear(256, 256)\n",
    "        #self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        #x = self.norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.norm2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu2(x)\n",
    "        #x = self.conv3(x)\n",
    "        #x = self.norm3(x)\n",
    "        #x = self.pool3(x)\n",
    "        #x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        #x = self.flatten(x)\n",
    "        x = torch.flatten(x)\n",
    "\n",
    "        # Replace LSTM part with this for simpler model (without sequence processing)\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.fc3(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dea03736",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5644aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53fc5d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(12, 12, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu2): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=4032, out_features=256, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (fc4): Linear(in_features=256, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('CurBest.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "908b6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "635e2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model(att_generator.__getitem__(0)[0][0].float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06b446c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActionToNumber(Action):\n",
    "    if len(Action) != 1: #For the shuffled case and evaluation\n",
    "        if Action[0] == 'N': #No Acceleration\n",
    "            return 0\n",
    "        elif Action[0] == 'S': #Small Acceleration\n",
    "            mult = 0\n",
    "        elif Action[0] == 'M': #Medium Acceleration\n",
    "            mult = 1\n",
    "        else: #Big Acceleration\n",
    "            mult = 2\n",
    "        Cardinal = Action[-2] + Action[-1]\n",
    "        Cardinal_list = [' N', 'NE', ' E', 'SE', ' S', 'SW', ' W', 'NW']\n",
    "        add = Cardinal_list.index(Cardinal) + 1\n",
    "        return 8 * mult + add\n",
    "    if Action[0][0] == 'N': #No Acceleration\n",
    "        return 0\n",
    "    elif Action[0][0] == 'S': #Small Acceleration\n",
    "        mult = 0\n",
    "    elif Action[0][0] == 'M': #Medium Acceleration\n",
    "        mult = 1\n",
    "    else: #Big Acceleration\n",
    "        mult = 2\n",
    "    Cardinal = Action[0][-2] + Action[0][-1]\n",
    "    Cardinal_list = [' N', 'NE', ' E', 'SE', ' S', 'SW', ' W', 'NW']\n",
    "    add = Cardinal_list.index(Cardinal) + 1\n",
    "    return 8 * mult + add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f80a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAdjacentActions(Action):\n",
    "    AdjacentActions = []\n",
    "    Cardinal_list = [' N', 'NE', ' E', 'SE', ' S', 'SW', ' W', 'NW']\n",
    "    if Action[0] == 'N': #No Acceleration\n",
    "        AdjacentActions += [0]\n",
    "        IntensityList = ['Small,']\n",
    "        for i in IntensityList:\n",
    "            for j in Cardinal_list:\n",
    "                AdjacentActions += [ActionToNumber(i+j)]\n",
    "        return AdjacentActions\n",
    "    elif Action[0] == 'S': #Small Acceleration\n",
    "        AdjacentActions += [0]\n",
    "        IntensityList = ['Small,', 'Medium,']\n",
    "    elif Action[0] == 'M': #Medium Acceleration\n",
    "        IntensityList = ['Small,', 'Medium,', 'Big,']\n",
    "    else:\n",
    "        IntensityList = ['Medium,', 'Big,']\n",
    "    Cardinal = Action[-2] + Action[-1]\n",
    "    idx = Cardinal_list.index(Cardinal)\n",
    "    if idx == 7:\n",
    "        AdjacentCardinals = [' W', 'NW', ' N']\n",
    "    else:\n",
    "        AdjacentCardinals = [Cardinal_list[idx-1], Cardinal_list[idx], Cardinal_list[idx+1]]\n",
    "    for i in IntensityList:\n",
    "        for j in AdjacentCardinals:\n",
    "            AdjacentActions += [ActionToNumber(i+j)]\n",
    "    return AdjacentActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "866161da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, reward, action, next_state, next_action, final_state):\n",
    "        action_index = ActionToNumber(action)\n",
    "        next_action_index = ActionToNumber(next_action)\n",
    "        Qvalue = output[action_index]\n",
    "        if not final_state:\n",
    "            output = model(next_state[0].float().to(device))\n",
    "            AdjacentActions = GetAdjacentActions(action[0])\n",
    "            OptionsValue = [output[j] for j in AdjacentActions]\n",
    "            next_Qvalue = max(OptionsValue)\n",
    "        else:\n",
    "            next_Qvalue = 0\n",
    "        reward = reward.to(device)\n",
    "        loss = (0.955*next_Qvalue + reward - Qvalue)**2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98c85c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CustomLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aac7a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflesampletrain(epochs):\n",
    "    size = 10000\n",
    "    samplevalidation(0)\n",
    "    lst = list(range(size))\n",
    "    for j in range(epochs):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        k = 0\n",
    "        random.shuffle(lst)\n",
    "        for i in lst:\n",
    "            cur_state = att_generator.__getitem__(i)[0]\n",
    "            cur_action = att_generator.__getitem__(i)[1]\n",
    "            cur_reward = torch.tensor(att_generator.__getitem__(i)[2])\n",
    "            next_state = att_generator.__getitem__(i+1)[0]\n",
    "            next_action = att_generator.__getitem__(i+1)[1]\n",
    "            data = cur_state[0].float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output,cur_reward,cur_action,next_state,next_action,cur_state[1])\n",
    "            with torch.no_grad():\n",
    "                train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            k += 1\n",
    "            prog = format((k/size) * 100, \".2f\")\n",
    "            print('Epoch',j,'progress:' + prog +'%',end='\\r')\n",
    "            if k == size:\n",
    "                break\n",
    "        with torch.no_grad():\n",
    "            train_loss /= size\n",
    "            print('\\nTrain loss is',train_loss)\n",
    "        samplevalidation(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99dc6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samplevalidation(j):\n",
    "    val_size = 3000\n",
    "    model.eval() \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for state, action, reward in val_def_dataloader:\n",
    "            if i == 0:\n",
    "                next_state = state\n",
    "                next_action = action\n",
    "                next_reward = reward\n",
    "                i += 1\n",
    "            else:\n",
    "                cur_state = state\n",
    "                cur_action = action\n",
    "                cur_reward = reward\n",
    "                data = cur_state[0].float().to(device)\n",
    "                output = model(data)\n",
    "                val_loss += loss_fn(output,cur_reward,cur_action,next_state,next_action,cur_state[1])\n",
    "                next_state = cur_state\n",
    "                next_action = cur_action\n",
    "                next_reward = cur_reward\n",
    "                i += 1\n",
    "                prog = format((i/val_size) * 100, \".2f\")\n",
    "                print('Validation',j,'progress:' + prog +'%',end='\\r')\n",
    "                if i == val_size:\n",
    "                    break\n",
    "        val_loss /= val_size\n",
    "        print('\\nValidation loss is',val_loss)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dab0bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampletrain(epochs):\n",
    "    size = 10000\n",
    "    samplevalidation(0)\n",
    "    for j in range(epochs):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for state, action, reward in def_dataloader:\n",
    "            if i == 0:\n",
    "                next_state = state\n",
    "                next_action = action\n",
    "                next_reward = reward\n",
    "                i += 1\n",
    "            else:\n",
    "                cur_state = state\n",
    "                cur_action = action\n",
    "                cur_reward = reward\n",
    "                data = cur_state[0].float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output,cur_reward,cur_action,next_state,next_action,cur_state[1])\n",
    "                with torch.no_grad():\n",
    "                    train_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                next_state = cur_state\n",
    "                next_action = cur_action\n",
    "                next_reward = cur_reward\n",
    "                i += 1\n",
    "                prog = format((i/size) * 100, \".2f\")\n",
    "                print('Epoch',j,'progress:' + prog +'%',end='\\r')\n",
    "                if i == size:\n",
    "                    break\n",
    "        with torch.no_grad():\n",
    "            train_loss /= size\n",
    "            print('\\nTrain loss is',train_loss)\n",
    "        samplevalidation(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb4df028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(j):\n",
    "    val_size = len(val_att_dataloader)\n",
    "    model.eval() \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for state, action, reward in val_att_dataloader:\n",
    "            if i == 0:\n",
    "                next_state = state\n",
    "                next_action = action\n",
    "                next_reward = reward\n",
    "                i += 1\n",
    "            else:\n",
    "                cur_state = state\n",
    "                cur_action = action\n",
    "                cur_reward = reward\n",
    "                data = cur_state[0].float().to(device)\n",
    "                output = model(data)\n",
    "                val_loss += loss_fn(output,cur_reward,cur_action,next_state,next_action,cur_state[1])\n",
    "                next_state = cur_state\n",
    "                next_action = cur_action\n",
    "                next_reward = cur_reward\n",
    "                i += 1\n",
    "                prog = format((i/val_size) * 100, \".2f\")\n",
    "                print('Validation',j,'progress:' + prog +'%',end='\\r')\n",
    "        val_loss /= val_size\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f3623d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    size = len(att_dataloader)\n",
    "    best_val_loss = validation(0)\n",
    "    print('\\nValidation loss is',best_val_loss)\n",
    "    print('')\n",
    "    counter = 0\n",
    "    for j in range(epochs):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for state, action, reward in att_dataloader:\n",
    "            if i == 0:\n",
    "                next_state = state\n",
    "                next_action = action\n",
    "                next_reward = reward\n",
    "                i += 1\n",
    "            else:\n",
    "                cur_state = state\n",
    "                cur_action = action\n",
    "                cur_reward = reward\n",
    "                data = cur_state[0].float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output,cur_reward,cur_action,next_state,next_action,cur_state[1])\n",
    "                with torch.no_grad():\n",
    "                    train_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                next_state = cur_state\n",
    "                next_action = cur_action\n",
    "                next_reward = cur_reward\n",
    "                i += 1\n",
    "                prog = format((i/size) * 100, \".2f\")\n",
    "                print('Epoch',j,'progress:' + prog +'%',end='\\r')\n",
    "        with torch.no_grad():\n",
    "            train_loss /= size\n",
    "            print('\\nTrain loss is',train_loss)\n",
    "        val_loss = validation(j)\n",
    "        print('\\nValidation loss is',val_loss)\n",
    "        print('')\n",
    "        if val_loss > best_val_loss:\n",
    "            counter += 1\n",
    "            if counter == 10:\n",
    "                break\n",
    "            torch.save(model.state_dict(), 'Cur.pt')\n",
    "        else:\n",
    "            counter = 0\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'CurBest.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7669d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation 0 progress:100.00%\n",
      "Validation loss is tensor([0.2412], device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Epoch 0 progress:100.00%\n",
      "Train loss is tensor([0.2311], device='cuda:0', dtype=torch.float64)\n",
      "Validation 0 progress:100.00%\n",
      "Validation loss is tensor([0.2410], device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Epoch 1 progress:100.00%\n",
      "Train loss is tensor([0.2310], device='cuda:0', dtype=torch.float64)\n",
      "Validation 1 progress:100.00%\n",
      "Validation loss is tensor([0.2408], device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Epoch 2 progress:87.70%\r"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639a5f2",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a5e9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9cfae",
   "metadata": {},
   "source": [
    "### New stuff, training on the def model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558616b",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c3658",
   "metadata": {},
   "source": [
    "0.000001 (256,25) 12 channels, dropout 0.3 - 0.1554\n",
    "\n",
    "0.0000001 (256,25) 12 channels, dropout 0.3 - \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5727f98",
   "metadata": {},
   "source": [
    "###### Validation 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3536a87",
   "metadata": {},
   "source": [
    "0.001 (256,25) 12 channels, dropout 0.3 - 0.1746\n",
    "\n",
    "0.001 (256,256,25) 12 channels, dropout 0.3 - 0.1796\n",
    "\n",
    "0.0001 (256,25) 12 channels - bad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4852e",
   "metadata": {},
   "source": [
    "#### Actual training results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a216f7e",
   "metadata": {},
   "source": [
    "0.000001 (256,25) 12 channels, dropout - 0.2269\n",
    "\n",
    "mixed (0.00001, 0.000001) (256,25) 12 channels, dropout 0.3 - 0.2190\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb797b65",
   "metadata": {},
   "source": [
    "##### Tests with 1000 validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1f371",
   "metadata": {},
   "source": [
    "0.0001 (256,25) 12 channels, dropout 0.1 - 0.1053\n",
    "\n",
    "0.0001 (256,25) 12 channels, dropout 0.2 - 0.1039"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4538b121",
   "metadata": {},
   "source": [
    "#### Tests with 3000 validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a35d6",
   "metadata": {},
   "source": [
    "0.0001 (256,25) 12 channels, dropout 0.2 - 0.2143\n",
    "\n",
    "0.001 (256,25) 12 channels, dropout 0.2 - 0.2154\n",
    "\n",
    "0.0001 (256,25) 12 channels, dropout 0.3 - 0.2104\n",
    "\n",
    "0.0001 (256,256, 25) 12 channels, dropout 0.3 - 0.2182"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da816f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d9ff2fb",
   "metadata": {},
   "source": [
    "0.00001 (512,256,25) 12 channels - 0.1232\n",
    "\n",
    "0.000001 (512,256,25) 12 channels - 0.1218\n",
    "\n",
    "0.000001 (256,25) 12 channels, dropout - 0.1201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611a8cc",
   "metadata": {},
   "source": [
    "Sample Train:\n",
    "\n",
    "0.000001 (256,25) 12 channels - 0.0949\n",
    "\n",
    "0.001 (256,25) 12 channels, dropout - 0.0939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f75e62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'AttModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "937c5da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0894,  0.1270,  0.1234, -0.1124, -0.0215, -0.0171, -0.0048, -0.1009,\n",
       "         0.1446,  0.3908,  0.1229, -0.1076, -0.0274,  0.2363, -0.0141, -0.0985,\n",
       "         0.1271,  0.1250,  0.1414, -0.1046, -0.0280, -0.0226, -0.0129, -0.1109,\n",
       "         0.1336], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(def_generator.__getitem__(0)[0][0].float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fa0dbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 Done\n",
      "1 1 2 Done\n",
      "1 1 3 Done\n",
      "1 1 4 Done\n",
      "1 1 5 Done\n",
      "1 1 6 Done\n",
      "1 1 7 Done\n",
      "1 1 8 Done\n",
      "1 1 9 Done\n",
      "1 1 10 Done\n",
      "1 1 11 Done\n",
      "1 2 1 Done\n",
      "1 2 2 Done\n",
      "1 2 3 Done\n",
      "1 2 4 Done\n",
      "1 2 5 Done\n",
      "1 2 6 Done\n",
      "1 2 7 Done\n",
      "1 2 8 Done\n",
      "1 2 9 Done\n",
      "1 2 10 Done\n",
      "1 2 11 Done\n",
      "2 1 1 Done\n",
      "2 1 2 Done\n",
      "2 1 3 Done\n",
      "2 1 4 Done\n",
      "2 1 5 Done\n",
      "2 1 6 Done\n",
      "2 1 7 Done\n",
      "2 1 8 Done\n",
      "2 1 9 Done\n",
      "2 1 10 Done\n",
      "2 1 11 Done\n",
      "2 2 1 Done\n",
      "2 2 2 Done\n",
      "2 2 3 Done\n",
      "2 2 4 Done\n",
      "2 2 5 Done\n",
      "2 2 6 Done\n",
      "2 2 7 Done\n",
      "2 2 8 Done\n",
      "2 2 9 Done\n",
      "2 2 10 Done\n",
      "2 2 11 Done\n",
      "3 1 1 Done\n",
      "3 1 2 Done\n",
      "3 1 3 Done\n",
      "3 1 4 Done\n",
      "3 1 5 Done\n",
      "3 1 6 Done\n",
      "3 1 7 Done\n",
      "3 1 8 Done\n",
      "3 1 9 Done\n",
      "3 1 10 Done\n",
      "3 1 11 Done\n",
      "3 2 1 Done\n",
      "3 2 2 Done\n",
      "3 2 3 Done\n",
      "3 2 4 Done\n",
      "3 2 5 Done\n",
      "3 2 6 Done\n",
      "3 2 7 Done\n",
      "3 2 8 Done\n",
      "3 2 9 Done\n",
      "3 2 10 Done\n",
      "3 2 11 Done\n",
      "4 1 1 Done\n",
      "4 1 2 Done\n",
      "4 1 3 Done\n",
      "4 1 4 Done\n",
      "4 1 5 Done\n",
      "4 1 6 Done\n",
      "4 1 7 Done\n",
      "4 1 8 Done\n",
      "4 1 9 Done\n",
      "4 1 10 Done\n",
      "4 1 11 Done\n",
      "4 2 1 Done\n",
      "4 2 2 Done\n",
      "4 2 3 Done\n",
      "4 2 4 Done\n",
      "4 2 5 Done\n",
      "4 2 6 Done\n",
      "4 2 7 Done\n",
      "4 2 8 Done\n",
      "4 2 9 Done\n",
      "4 2 10 Done\n",
      "4 2 11 Done\n",
      "5 1 1 Done\n",
      "5 1 2 Done\n",
      "5 1 3 Done\n",
      "5 1 4 Done\n",
      "5 1 5 Done\n",
      "5 1 6 Done\n",
      "5 1 7 Done\n",
      "5 1 8 Done\n",
      "5 1 9 Done\n",
      "5 1 10 Done\n",
      "5 1 11 Done\n",
      "5 2 1 Done\n",
      "5 2 2 Done\n",
      "5 2 3 Done\n",
      "5 2 4 Done\n",
      "5 2 5 Done\n",
      "5 2 6 Done\n",
      "5 2 7 Done\n",
      "5 2 8 Done\n",
      "5 2 9 Done\n",
      "5 2 10 Done\n",
      "5 2 11 Done\n",
      "6 1 1 Done\n",
      "6 1 2 Done\n",
      "6 1 3 Done\n",
      "6 1 4 Done\n",
      "6 1 5 Done\n",
      "6 1 6 Done\n",
      "6 1 7 Done\n",
      "6 1 8 Done\n",
      "6 1 9 Done\n",
      "6 1 10 Done\n",
      "6 1 11 Done\n",
      "6 2 1 Done\n",
      "6 2 2 Done\n",
      "6 2 3 Done\n",
      "6 2 4 Done\n",
      "6 2 5 Done\n",
      "6 2 6 Done\n",
      "6 2 7 Done\n",
      "6 2 8 Done\n",
      "6 2 9 Done\n",
      "6 2 10 Done\n",
      "6 2 11 Done\n",
      "7 1 1 Done\n",
      "7 1 2 Done\n",
      "7 1 3 Done\n",
      "7 1 4 Done\n",
      "7 1 5 Done\n",
      "7 1 6 Done\n",
      "7 1 7 Done\n",
      "7 1 8 Done\n",
      "7 1 9 Done\n",
      "7 1 10 Done\n",
      "7 1 11 Done\n",
      "7 2 1 Done\n",
      "7 2 2 Done\n",
      "7 2 3 Done\n",
      "7 2 4 Done\n",
      "7 2 5 Done\n",
      "7 2 6 Done\n",
      "7 2 7 Done\n",
      "7 2 8 Done\n",
      "7 2 9 Done\n",
      "7 2 10 Done\n",
      "7 2 11 Done\n"
     ]
    }
   ],
   "source": [
    "PerformanceList = []\n",
    "length = len(DefEvaluationList)\n",
    "with torch.no_grad():\n",
    "    for Match in range(1,8):\n",
    "        for Team in range(1,3):\n",
    "            for Player in range(1,12):\n",
    "                impact = 0\n",
    "                Evaldf = DefEvaluationList[(DefEvaluationList['Match_index'] == Match) & (DefEvaluationList['Agent Team'] == Team) & (DefEvaluationList['Agent Player'] == Player)]\n",
    "                for i in Evaldf.index:\n",
    "                    idx = length - i - 1\n",
    "                    state, action, _ = eval_def_generator.__getitem__(idx)\n",
    "                    if Evaldf.loc[i][-1]:\n",
    "                        output = model(eval_def_generator.__getitem__(idx)[0][0].float().to(device))\n",
    "                        ChoiceValue = output[ActionToNumber(action)]\n",
    "                        impact += ChoiceValue - (sum(output)/len(output))\n",
    "                    else:\n",
    "                        _, prev_action, _ = eval_def_generator.__getitem__(idx+1)\n",
    "                        AdjacentActions = GetAdjacentActions(prev_action)\n",
    "                        output = model(eval_def_generator.__getitem__(idx)[0][0].float().to(device))\n",
    "                        OptionsValue = [output[j] for j in AdjacentActions]\n",
    "                        ChoiceValue = output[ActionToNumber(action)]\n",
    "                        impact += ChoiceValue - (sum(OptionsValue)/len(OptionsValue))\n",
    "                impact /= len(Evaldf)\n",
    "                PerformanceList += [[Match,Team,Player,impact]]\n",
    "                print(Match,Team,Player,'Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1da2c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceDataframe = pd.DataFrame(PerformanceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d12f4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(PerformanceDataframe):\n",
    "    PerformanceDataframe.iloc[i,-1]  = PerformanceDataframe.iloc[i,-1].item()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87e9621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceDataframe.columns = ['Match','Team', 'Player', 'Impact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14f71180",
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceDataframe.to_csv('PerformanceDataframeDef.csv',header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96887e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match</th>\n",
       "      <th>Team</th>\n",
       "      <th>Player</th>\n",
       "      <th>Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.019326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.024337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.017073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.013593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.024657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.024802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.017199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.029799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.020608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.027888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Match  Team  Player    Impact\n",
       "0        1     1       1 -0.019326\n",
       "1        1     1       2 -0.024337\n",
       "2        1     1       3 -0.017073\n",
       "3        1     1       4 -0.013593\n",
       "4        1     1       5 -0.024657\n",
       "..     ...   ...     ...       ...\n",
       "149      7     2       7 -0.024802\n",
       "150      7     2       8 -0.017199\n",
       "151      7     2       9 -0.029799\n",
       "152      7     2      10 -0.020608\n",
       "153      7     2      11 -0.027888\n",
       "\n",
       "[154 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PerformanceDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb6a4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "U15Dataframe = PerformanceDataframe[PerformanceDataframe['Match'] <= 3]\n",
    "U17Dataframe = PerformanceDataframe[(PerformanceDataframe['Match'] <= 5) & (PerformanceDataframe['Match'] >= 4)]\n",
    "U19Dataframe = PerformanceDataframe[PerformanceDataframe['Match'] >= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0959b87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 score is: -0.025483633137562058\n",
      "Game 2 score is: -0.022453765173188665\n",
      "Game 3 score is: -0.028096942612054674\n",
      "Game 4 score is: -0.03575908155603842\n",
      "Game 5 score is: -0.03646235582842068\n",
      "Game 6 score is: -0.02555742889473384\n",
      "Game 7 score is: -0.018068839838220316\n"
     ]
    }
   ],
   "source": [
    "Dataframe1 = PerformanceDataframe[PerformanceDataframe['Match'] == 1]\n",
    "Dataframe2 = PerformanceDataframe[PerformanceDataframe['Match'] == 2]\n",
    "Dataframe3 = PerformanceDataframe[PerformanceDataframe['Match'] == 3]\n",
    "Dataframe4 = PerformanceDataframe[PerformanceDataframe['Match'] == 4]\n",
    "Dataframe5 = PerformanceDataframe[PerformanceDataframe['Match'] == 5]\n",
    "Dataframe6 = PerformanceDataframe[PerformanceDataframe['Match'] == 6]\n",
    "Dataframe7 = PerformanceDataframe[PerformanceDataframe['Match'] == 7]\n",
    "Score1 = Dataframe1.sum()[-1]/len(Dataframe1)\n",
    "Score2 = Dataframe2.sum()[-1]/len(Dataframe2)\n",
    "Score3 = Dataframe3.sum()[-1]/len(Dataframe3)\n",
    "Score4 = Dataframe4.sum()[-1]/len(Dataframe4)\n",
    "Score5 = Dataframe5.sum()[-1]/len(Dataframe5)\n",
    "Score6 = Dataframe6.sum()[-1]/len(Dataframe6)\n",
    "Score7 = Dataframe7.sum()[-1]/len(Dataframe7)\n",
    "print('Game 1 score is:',Score1)\n",
    "print('Game 2 score is:',Score2)\n",
    "print('Game 3 score is:',Score3)\n",
    "print('Game 4 score is:',Score4)\n",
    "print('Game 5 score is:',Score5)\n",
    "print('Game 6 score is:',Score6)\n",
    "print('Game 7 score is:',Score7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "494fe0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U15 score is: -0.0253447803076018\n",
      "U17 score is: -0.03611071869222955\n",
      "U19 score is: -0.02181313436647708\n"
     ]
    }
   ],
   "source": [
    "U15Score = U15Dataframe.sum()[-1]/len(U15Dataframe)\n",
    "U17Score = U17Dataframe.sum()[-1]/len(U17Dataframe)\n",
    "U19Score = U19Dataframe.sum()[-1]/len(U19Dataframe)\n",
    "print('U15 score is:',U15Score)\n",
    "print('U17 score is:',U17Score)\n",
    "print('U19 score is:',U19Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45cb18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTeamScores(Dataframe):\n",
    "    Team1Score = 0\n",
    "    Team2Score = 0\n",
    "    for i in range(len(Dataframe)):\n",
    "        if Dataframe.iloc[i][1] == 1:\n",
    "            Team1Score += Dataframe.iloc[i][-1]\n",
    "        else:\n",
    "            Team2Score += Dataframe.iloc[i][-1]\n",
    "    print(Team1Score,Team2Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eaabc6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2810705006122589 -0.27956942841410637\n",
      "-0.25771714374423027 -0.23626569006592035\n",
      "-0.33122526854276657 -0.28690746892243624\n",
      "-0.41126235388219357 -0.37543744035065174\n",
      "-0.3867097236216068 -0.4154621046036482\n",
      "-0.29826067201793194 -0.26400276366621256\n",
      "-0.17087524756789207 -0.22663922887295485\n"
     ]
    }
   ],
   "source": [
    "GetTeamScores(Dataframe1)\n",
    "GetTeamScores(Dataframe2)\n",
    "GetTeamScores(Dataframe3)\n",
    "GetTeamScores(Dataframe4)\n",
    "GetTeamScores(Dataframe5)\n",
    "GetTeamScores(Dataframe6)\n",
    "GetTeamScores(Dataframe7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5da5c6",
   "metadata": {},
   "source": [
    "### Scores:\n",
    "\n",
    "Home 0 - 1 Away\n",
    "\n",
    "Home 8 - 1 Away\n",
    "\n",
    "Home 4 - 3 Away\n",
    "\n",
    "Home 7 - 2 Away\n",
    "\n",
    "Home 2 - 0 Away\n",
    "\n",
    "Home 1 - 0 Away\n",
    "\n",
    "Home 4 - 3 Away"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83778849",
   "metadata": {},
   "source": [
    "Code to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningDataGenerator(Dataset):\n",
    "    def __init__(self, reward_list):\n",
    "        self.reward_list = reward_list\n",
    "        # Load dataFrames outside the constructor for efficiency\n",
    "        #self.PlayerPos = pd.read_csv('PlayerPos.csv')\n",
    "        #self.Home = pd.read_csv('Home.csv')\n",
    "        #self.Away = pd.read_csv('Away.csv')\n",
    "        #self.VelHome = pd.read_csv('VelHome.csv')\n",
    "        #self.VelAway = pd.read_csv('VelAway.csv')\n",
    "        #self.AccHome = pd.read_csv('DiscAccHome.csv')\n",
    "        #self.AccAway = pd.read_csv('DiscAccAway.csv')\n",
    "        #self.Ball = pd.read_csv('Ball.csv')\n",
    "        #self.VelBall = pd.read_csv('VelBall.csv')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reward_list.loc[self.reward_list['Match_index'] != 0])\n",
    "    def __getitem__(self, idx):\n",
    "        No_zeros = self.reward_list.loc[self.reward_list['Match_index'] != 0]\n",
    "        Match_index, Frame, Team, Player, AgentTeam, AgentPlayer, reward = No_zeros.iloc[idx]\n",
    "        original_id = No_zeros.index[idx]\n",
    "        final_state = self.reward_list.iloc[original_id + 1][0] == 0  # Check next row for episode end\n",
    "        state = self.get_state(Match_index, Frame, Team, Player, AgentTeam, AgentPlayer, final_state)\n",
    "        action = self.get_action(Match_index, Frame, Team, Player, AgentTeam, AgentPlayer)\n",
    "        return state, action, reward\n",
    "\n",
    "    def get_state(self, Match_index, Frame,Team,Player,AgentTeam,AgentPlayer, final_state):\n",
    "        n = FrameToIndex(Match_index, Frame)\n",
    "        MatOpp = np.zeros([60,100])\n",
    "        MatTeam = np.zeros([60,100])\n",
    "        MatOppvx = np.zeros([60,100])\n",
    "        MatOppvy = np.zeros([60,100])\n",
    "        MatTeamvx = np.zeros([60,100])\n",
    "        MatTeamvy = np.zeros([60,100])\n",
    "        MatBall = np.zeros([60,100])\n",
    "        MatBallvx = np.zeros([60,100])\n",
    "        MatBallvy = np.zeros([60,100])\n",
    "        MatAge = np.zeros([60,100])\n",
    "        MatAgevx = np.zeros([60,100])\n",
    "        MatAgevy = np.zeros([60,100])\n",
    "        xball = Ball.iloc[n,2]\n",
    "        yball = Ball.iloc[n,3]\n",
    "        MatBall[yball][xball] = 1\n",
    "        MatBallvx[yball][xball] = VelBall.iloc[n,2]\n",
    "        MatBallvy[yball][xball] = VelBall.iloc[n,3]\n",
    "        if AgentTeam==1:\n",
    "            yopp = Away.iloc[n, [2*i+2 for i in range(1, 12)]].to_numpy()\n",
    "            xopp = Away.iloc[n, [2*i+1 for i in range(1, 12)]].to_numpy()\n",
    "            yteam = Home.iloc[n, [2*i+2 for i in range(1, 12) if i != AgentPlayer]].to_numpy()\n",
    "            xteam = Home.iloc[n, [2*i+1 for i in range(1, 12) if i != AgentPlayer]].to_numpy()\n",
    "            yopp = yopp.round().astype(int)  # Round and convert to integer\n",
    "            xopp = xopp.round().astype(int)\n",
    "            yteam = yteam.round().astype(int)\n",
    "            xteam = xteam.round().astype(int)\n",
    "            xvelteam = VelHome.iloc[n,[2*i for i in range(1,12) if i != AgentPlayer]].to_numpy()\n",
    "            yvelteam = VelHome.iloc[n,[2*i+1 for i in range(1,12) if i != AgentPlayer]].to_numpy()\n",
    "            xvelopp = VelAway.iloc[n,[2*i for i in range(1,12)]].to_numpy()\n",
    "            yvelopp = VelAway.iloc[n,[2*i+1 for i in range(1,12)]].to_numpy()\n",
    "            MatOpp[yopp, xopp] = 1\n",
    "            MatOppvx[yopp, xopp] = xvelopp\n",
    "            MatOppvy[yopp, xopp] = yvelopp\n",
    "            MatTeam[yteam, xteam] = 1\n",
    "            MatTeamvx[yteam, xteam] = xvelteam\n",
    "            MatTeamvy[yteam, xteam] = yvelteam\n",
    "            yage = Home.iloc[n,2*AgentPlayer + 2]\n",
    "            xage = Home.iloc[n,2*AgentPlayer + 1]\n",
    "            xvelage = VelHome.iloc[n,2*AgentPlayer]\n",
    "            yvelage = VelHome.iloc[n,2*AgentPlayer + 1]\n",
    "            MatAge[yage, xage] = 1\n",
    "            MatAgevx[yage, xage] = xvelage\n",
    "            MatAgevy[yage, xage] = yvelage\n",
    "        if AgentTeam==2:\n",
    "            yopp = Home.iloc[n, [2*i+2 for i in range(1, 12)]].to_numpy()\n",
    "            xopp = Home.iloc[n, [2*i+1 for i in range(1, 12)]].to_numpy()\n",
    "            yteam = Away.iloc[n, [2*i+2 for i in range(1, 12) if i != AgentPlayer]].to_numpy()\n",
    "            xteam = Away.iloc[n, [2*i+1 for i in range(1, 12) if i != AgentPlayer]].to_numpy()\n",
    "            yopp = yopp.round().astype(int)  # Round and convert to integer\n",
    "            xopp = xopp.round().astype(int)\n",
    "            yteam = yteam.round().astype(int)\n",
    "            xteam = xteam.round().astype(int)\n",
    "            xvelteam = VelAway.iloc[n,[2*i for i in range(1,12) if i != AgentPlayer]].to_numpy()\n",
    "            yvelteam = VelAway.iloc[n,[2*i+1 for i in range(1,12) if i != AgentPlayer]].to_numpy()\n",
    "            xvelopp = VelHome.iloc[n,[2*i for i in range(1,12)]].to_numpy()\n",
    "            yvelopp = VelHome.iloc[n,[2*i+1 for i in range(1,12)]].to_numpy()\n",
    "            MatOpp[yopp, xopp] = 1\n",
    "            MatOppvx[yopp, xopp] = xvelopp\n",
    "            MatOppvy[yopp, xopp] = yvelopp\n",
    "            MatTeam[yteam, xteam] = 1\n",
    "            MatTeamvx[yteam, xteam] = xvelteam\n",
    "            MatTeamvy[yteam, xteam] = yvelteam\n",
    "            yage = Away.iloc[n,2*AgentPlayer + 2]\n",
    "            xage = Away.iloc[n,2*AgentPlayer + 1]\n",
    "            xvelage = VelAway.iloc[n,2*AgentPlayer]\n",
    "            yvelage = VelAway.iloc[n,2*AgentPlayer + 1]\n",
    "            MatAge[yage, xage] = 1\n",
    "            MatAgevx[yage, xage] = xvelage\n",
    "            MatAgevy[yage, xage] = yvelage\n",
    "        MatOpp = torch.from_numpy(MatOpp)\n",
    "        MatTeam = torch.from_numpy(MatTeam)\n",
    "        MatOppvx = torch.from_numpy(MatOppvx)\n",
    "        MatOppvy = torch.from_numpy(MatOppvy)\n",
    "        MatTeamvx = torch.from_numpy(MatTeamvx)\n",
    "        MatTeamvy = torch.from_numpy(MatTeamvy)\n",
    "        MatBall = torch.from_numpy(MatBall)\n",
    "        MatBallvx = torch.from_numpy(MatBallvx)\n",
    "        MatBallvy = torch.from_numpy(MatBallvy)\n",
    "        MatAge = torch.from_numpy(MatAge)\n",
    "        MatAgevx = torch.from_numpy(MatAgevx)\n",
    "        MatAgevy = torch.from_numpy(MatAgevy)\n",
    "        return [torch.stack([MatOpp,MatTeam,MatOppvx,MatOppvy,MatTeamvx,MatTeamvy,MatBall,MatBallvx,MatBallvy,MatAge,MatAgevx,MatAgevy]), final_state]\n",
    "\n",
    "\n",
    "  # Add a method to get action if applicable for your Q-learning approach\n",
    "    def get_action(self, Match_index, Frame, Team,Player,AgentTeam,AgentPlayer):\n",
    "        n = FrameToIndex(Match_index, Frame)\n",
    "        if AgentTeam==1:\n",
    "            Magnitude = DiscAccHome.iloc[n,2*AgentPlayer]\n",
    "            Direction = DiscAccHome.iloc[n,2*AgentPlayer+1]\n",
    "        if AgentTeam==2:\n",
    "            Magnitude = DiscAccAway.iloc[n,2*AgentPlayer]\n",
    "            Direction = DiscAccAway.iloc[n,2*AgentPlayer+1]\n",
    "        if Magnitude < 0.2:\n",
    "            return 'No Acceleration'\n",
    "        elif Magnitude < 0.5:\n",
    "            return 'Small, '+Direction\n",
    "        elif Magnitude < 1:\n",
    "            return 'Medium, '+Direction\n",
    "        else:\n",
    "            return 'Big, '+Direction\n",
    "        return (Magnitude,Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    size = len(att_dataloader)\n",
    "    #validation(0)\n",
    "    for j in range(epochs):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for state, action, reward in att_dataloader:\n",
    "            if i == 0:\n",
    "                cur_state = state\n",
    "                cur_action = action\n",
    "                cur_reward = reward\n",
    "                i += 1\n",
    "            else:\n",
    "                next_state = state\n",
    "                next_action = action\n",
    "                next_reward = reward\n",
    "                data = cur_state[0].float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output,cur_reward,cur_action,next_state,next_action,cur_state[1])\n",
    "                with torch.no_grad():\n",
    "                    train_loss += loss\n",
    "                    print(train_loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                cur_state = next_state\n",
    "                cur_action = next_action\n",
    "                cur_reward = next_reward\n",
    "                i += 1\n",
    "                prog = format((i/size) * 100, \".2f\")\n",
    "                print('Epoch',j,'progress:' + prog +'%',end='\\r')\n",
    "        with torch.no_grad():\n",
    "            train_loss /= size\n",
    "            print('\\nTrain loss is',train_loss)\n",
    "        model.eval() \n",
    "        validation(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(j):\n",
    "    val_size = len(val_att_dataloader)\n",
    "    model.eval() \n",
    "    val_loss = 0\n",
    "    print('')\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for state, action, reward in val_att_dataloader:\n",
    "            if i == 0:\n",
    "                cur_state = state\n",
    "                cur_action = action\n",
    "                cur_reward = reward\n",
    "                i += 1\n",
    "            else:\n",
    "                next_state = state\n",
    "                next_action = action\n",
    "                next_reward = reward\n",
    "                data = cur_state[0].float().to(device)\n",
    "                output = model(data)\n",
    "                val_loss += loss_fn(output,cur_reward,cur_action,next_state,next_action,cur_state[1])\n",
    "                cur_state = next_state\n",
    "                cur_action = next_action\n",
    "                cur_reward = next_reward\n",
    "                i += 1\n",
    "                prog = format((i/val_size) * 100, \".2f\")\n",
    "                print('Validation',j,'progress:' + prog +'%',end='\\r')\n",
    "        val_loss /= val_size\n",
    "        print('\\nValidation loss is',val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28219cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceList = []\n",
    "length = len(DefEvaluationList)\n",
    "with torch.no_grad():\n",
    "    for Match in range(1,8):\n",
    "        for Team in range(1,3):\n",
    "            for Player in range(1,12):\n",
    "                impact = 0\n",
    "                Evaldf = DefEvaluationList[(DefEvaluationList['Match_index'] == Match) & (DefEvaluationList['Agent Team'] == Team) & (DefEvaluationList['Agent Player'] == Player)]\n",
    "                for i in Evaldf.index:\n",
    "                    idx = length - i - 1\n",
    "                    state, action, _ = eval_def_generator.__getitem__(idx)\n",
    "                    if Evaldf.loc[i][-1]:\n",
    "                        output = model(eval_def_generator.__getitem__(idx)[0][0].float().to(device))\n",
    "                        ChoiceValue = output[ActionToNumber(action)]\n",
    "                        impact += ChoiceValue - (sum(output)/len(output))\n",
    "                    else:\n",
    "                        _, prev_action, _ = eval_def_generator.__getitem__(idx+1)\n",
    "                        AdjacentActions = GetAdjacentActions(prev_action)\n",
    "                        output = model(eval_def_generator.__getitem__(idx)[0][0].float().to(device))\n",
    "                        OptionsValue = [output[j] for j in AdjacentActions]\n",
    "                        ChoiceValue = output[ActionToNumber(action)]\n",
    "                        impact += ChoiceValue - (sum(OptionsValue)/len(OptionsValue))\n",
    "                impact /= len(Evaldf)\n",
    "                PerformanceList += [[Match,Team,Player,impact]]\n",
    "                print(Match,Team,Player,'Done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
